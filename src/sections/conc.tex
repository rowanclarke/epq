\section{Conclusion}
\label{conc}

\subsection{Summary}

We have seen the correlation between machine learning and human learning, \gls{model}ling learning through the \gls{perceptron}, understanding how learning occurs through back-propagation, and how we can expand this knowledge to create specific architectures and \gls{model}s.

With these ideas, we were able to find limitations to given problems, and what is required for a machine to learn. Once we have arrived at a clear \gls{model} for solving problems, we then discussed the ethics and legality of doing so.

\subsection{Main Conclusion}

It is clear to see that with a \gls{model} that does not have any assumptions, enough data, and a \gls{model} complexity that mirrors the complexity of the problem, we can teach a machine any problem through \gls{decomposition} and our own understanding, to influence the machine to learn a pattern.

It should therefore be considered if it is legal or ethically possible, which under certain conditions, produces moral concerns.

\subsection{Recommendations for Further Work}

There are numerous areas where further research was necessary to produce a clear argument for how more complex architectures are designed and used. This includes researching and designing different activations, different methods of back-propagation, and complex problems that require decomposition and \gls{decomposition} of multiple neural architectures.

